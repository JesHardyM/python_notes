Set up Docker
Docker is an easy way to package and run applications in containers. Some would consider it the most popular containerized technology. A container is a lightweight, portable, and isolated environment that facilitates the testing and deployment of new software. Within the container, the application is isolated from all other processes on the host machine. In the programming world, there is a saying that goes, “Well, it works on my machine,” meaning that a developer wrote some code that works perfectly on their local machine but does not work on others’ machines. Docker helps solve this common—and annoying—problem by providing a consistent runtime across different environments.

In this reading, you will learn more about Docker, including how to install it, and you’ll receive step-by-step instructions along the way.

Parts of Docker 
The Docker ecosystem consists of the following parts:

Docker daemon. This manages running containers on a host machine called the Docker Host.

Docker CLI (command-line interface). This command-line tool interacts with Docker Daemon. 

Docker Desktop. This graphical user interface (GUI) tool interacts with the daemon.

Docker Hub. This is the central repository for downloading containers.

You might hear the host machine referred to as Docker Host. Docker uses a client-server architecture as outlined by the image below. Docker supports running the client tools and daemon on different machines. This is an advantage of Docker as it allows you to manage containers on a remote server as easily as if they’re on your own workstation. 

Three separate groups titled client, Docker host, and registry. Docker run, Docker build, and Docker pull are all listedunder client and have arrows pointing to Docker daemon, which is listed under Docker host. Docker daemon contains arrows pointing to Images, which are listed under registry. The Images listed under registry have an arrow pointing to the images that are listed under Docker host.
Installing Docker 
Before you get started, it is recommended you read the 
Getting started guide
 in the official Docker documentation. Next, download and install Docker according to your operating system:

Windows. 
Install Docker desktop on Windows | Docker documentation

macOS. 
Install Docker desktop on Mac | Docker documentation

Linux. 
Install Docker desktop on Linux | Docker documentation

When you complete the installation, you will have the Docker Daemon and Docker Desktop app installed. You are now ready to run your first container. Let’s look at how to do this on the Docker desktop app:

Open the Docker desktop app.

Select the Search bar at the top of the window.

In the search bar, type hello-world and press Enter.

The Docker desktop app opened with hello-world typed into the Search bar at the top of the window.
To the right of the found hello-world container, click Run.

The Docker desktop app opened with hello-world displayed as a search result and Run as a command option on the far rightside of the screen.
Docker downloads the image from Docker Hub and runs it. If successful, you will see a congratulatory message:

Hello from Docker!

This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:

The Docker client contacted the Docker daemon.

The Docker daemon pulled the "hello-world" image from the Docker Hub. (amd64)

The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading.

The Docker daemon streamed that output to the Docker client, which sent it to your terminal.

To try something more ambitious, you can run an Ubuntu container with:

$ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:

https://hub.docker.com/

For more examples and ideas, visit:

https://docs.docker.com/get-started/

And that’s it! Not too bad right? Now let’s do the same thing, but this time from the command line.

Open a terminal window, type docker, and press Enter.

Note: The Docker CLI displays a summary of possible subcommands when the installation is successful.

Type docker run hello-world and press Enter.

Note: Docker displays diagnostic messages about downloading the hello-world image, then the following message if successful:

Hello from Docker!

This message shows that your installation appears to be working correctly.

Troubleshooting
You might run into a situation in which your permissions are denied while trying to connect to the Docker daemon. In this case, run the command again with sudo:

sudo docker run hello-world

If that command is successful, check your OS for a Docker group and add yourself to it. This allows you to run Docker commands without being rooted.

The Docker installation should automatically add the Docker CLI to your system path. Unfortunately, that doesn’t always happen. You’ll know that the Docker installation did not add the Docker CLI to your system path if you receive the message: command not found: docker. In this case, log out and back in to see if it corrects the issue. If not, consult your OS documentation on how to find the Docker binary and change your path. For additional help for troubleshooting Docker installation, view the links according to your operating system:

Linux: 
Troubleshoot Docker engine installation

Window and Mac: 
Workarounds for common problems

Key takeaways
When you have Docker up and running on your machine, you are in business! Docker provides so many advantages. With Docker, you can package your application and everything it needs into a portable container, and run the container! The best part is you can run the container from almost anywhere; it does not have to be on your own workstation.

______________________________________________________

For Docker web apps, you upload the container to a server so people can connect to it remotely. In the Dockerfile, you would specify that it's a web-enabled application. An example of a Docker web app is one where you were running a Docker image, like a database, on your computer, and it allows traffic from other computers as well. When you share a container with a Dockerfile, it's important to set expectations with the individual or individuals you're sharing the container with. This is especially true if they aren't familiar with Docker. The first time you run a Dockerfile, it has to download and install the Python runtime file for the version of Python used to create the application, but only once. If you share other containers with those same parameters, it will simply run the Python script. Since Docker is open source, you can use it for free. There aren't any licensing requirements, but there is a Docker version that offers enterprise and subscription plans for a fee that will save your Docker images on a private cloud and allow you to manage all of your Docker images. But buying a plan is only one option. Docker support is built into the Google Cloud Platform, so you can simply upload your images there. In addition, many network storage devices and cloud backup servers like Buffalo and Western Digital include support for Docker. Although Docker is a relatively new tool, it has become practically synonymous with creating containers, mostly because it's simpler than the other methods for creating containers, like using a virtual machine or setting up a virtual environment like VM or Conda.

Docker images are the building blocks of Docker containers. They are lightweight, immutable, and composed of multiple layers. A Docker image contains the application code, data files, configuration files, libraries, and other dependencies needed to run an application.

In this reading, you will learn more about Docker images and their layers. You’ll also learn how to build a Docker image, and you’ll review an example of a Dockerfile.

Docker images and image layers
You can think of a Docker image as a template from which Docker containers are created and executed. Each Docker image is composed of multiple layers—adding or removing files from the previous layer. Each layer represents a specific set of changes made to the image and is composed based on the instructions in a Dockerfile. The instructions in a Dockerfile define how the image should be built.

Note: It’s not uncommon for an image to be composed of a dozen or more layers.

The purpose of having multiple layers is to keep the final images as small as possible—you do this by reusing layers in multiple images—and to speed up the process of building containers, as Docker has to rebuild only the layers that have changed.

How to build a Docker image
The key to packaging your own application as a Docker image is to have a Dockerfile. The Dockerfile acts as your source of truth or instruction manual: It specifies how Docker should build the image and contains a series of commands to build the image. Each command builds a new layer that becomes part of the final image. A common process is to start with a base image such as Debian Linux or Python 3.10, install the libraries your application requires, then copy the application and any related files into the image. Let’s take a look at a simple Dockerfile for a Python application example:


FROM python:3.9

This line of code says that you’re starting from the Python 3.9 base image.


COPY *.py setup.cfg LICENSE README.md requirements.txt /app/

WORKDIR /app

This command says to copy all of the application’s files to a folder inside the container named /app and make it the current working directory.


RUN pip install -r requirements.txt

RUN python setup.py install

These two lines of code run the Python commands to install the libraries required by the app. When that step is complete, build and install the app inside the container.


EXPOSE 8000

CMD [ "/usr/local/bin/my-application" ]

This command tells Docker what executable should run when the container starts and that the container will listen for network connections on port 8000.


Pro tip: To build this image from the Dockerfile, use the command: docker build. If the build is successful, Docker outputs the ID of the new image, which you can then use to start a container.

Refer to the 
Dockerfile reference
 for a full list of commands that can appear in a Dockerfile.

Image names, tags, and IDs
You use tags and IDs to identify and reference Docker images. Their unique names provide a way to differentiate between specific versions of Docker images. The ID is a random string of numbers and letters, which most of the time are way too complicated to remember. But there’s good news! You can assign any number of tags to the image, in addition to the ID. Tags are alphanumeric labels that help users find the correct image. Most images are tagged with the author’s Github username, the name of the application, and a version number. 


Pro tip: Tag the most recent version of an image with latest in addition to a version number. This makes it easy for people to find the current version of your application.


Let’s look at an example:

csmith/my-docker-image:1.0

csmith/my-docker-image:latest 

sha256:abc123def456


csmith is the name of the author, my-docker-image is the image name, 1.0 is the version number (and it’s the latest version), and sha256:abc123def456 represents the image ID.

How to manage images
A great thing about Docker is that it caches images on a disk. Therefore, you don’t need to go grab them or rebuild them every time you need them. This saves you so much time! Some of the Docker CLI (command line interface) commands you can use include:

docker image ls – This command lists the images cached locally.

docker image tag – This command applies tags to a local image.

docker image pull – This command fetches an image from a remote repository.

docker image push – This command sends a local image to a remote repository.

docker image rm – This command removes an image from the cache.

docker image prune – This command removes all unused images to reclaim disk space.

Key takeaways
Docker images—including tags and IDs—are essential for programmers to package, distribute, and deploy applications more efficiently, reducing issues and improving the stages of the software workflow. Remember, in order to have a Docker image, you must have a Dockerfile. These components work hand-in-hand; you can’t have one without the other.

_________________________________
Using multiple containers
Imagine you are developing a web-based platform that allows users to browse products, add items to their cart, pay for items, and ship items to different addresses. This application requires multiple components to execute properly because it relies on a number of microservices. The idea behind microservices is to take a large application and break it up into smaller, more tangible, independent parts of the application that are self-contained. This allows for each part of the application to be better maintained. Because these microservices are independent of each other, you use multiple containers to test the entirety of the application to ensure everything runs smoothly. It’s no surprise that in the programming world, programmers and developers work with multiple containers at a time.

In this reading, you will learn more about the use of multiple containers, commands for working with multiple containers, how related services find each other, and how to install Docker Compose and view an example.

Starting multiple containers
To start multiple containers, you need to run multiple docker run commands. A docker run command creates a container and starts it. Let’s look at an example of how to create and start two containers that work together once they find each other by name.

As a programmer, you’ve been asked to set up a WordPress blog. You know WordPress requires a database to store its content. You create and start two containers, wordpress and db, using the following command:

$ docker run -d --name db --restart always \

    -v db_data:/var/lib/mysql -p 3306 -p 33060 \

    -e MYSQL_ROOT_PASSWORD=somewordpress \

    -e MYSQL_DATABASE=wordpress \

    -e MYSQL_USER=wordpress \

    -e MYSQL_PASSWORD=wordpress \

    mariadb:10

This command starts the mariadb database, determines a storage volume, and sets the initial password for the WordPress user. It declares two network ports open to other containers, but it is not shown on the host machine.

Now, start the WordPress container using the following command:

$ docker run -d --name wordpress --restart always \

    -v wp_data:/var/www/html -p 80:80 \

    -e WORDPRESS_DB_HOST=db \

    -e WORDPRESS_DB_USER=wordpress \

    -e WORDPRESS_DB_PASSWORD=wordpress \

    -e WORDPRESS_DB_NAME=wordpress \

    wordpress:latest

Note: The environment variable WORDPRESS_DB_HOST is set to db on the third line. This line of code is needed to refer to another container. Docker provides domain name system (DNS) services that allow containers to find each other by their name.

Networking with multiple containers
Imagine you have several customers using the same application. For security reasons, you have isolated the application and created multiple containers, one for each customer. Docker allows you to create private networks for a container or groups of containers. These private containers are able to discover each other, but no other networks will be able to find the private containers you’ve started. Let’s look at an example: modifying the wordpress and db containers by putting them on a private network.

First, stop and delete both containers:

$ docker stop wordpress && docker rm wordpress

$ docker stop db && docker rm db

Then, create a private network for both containers to use:

$ docker network create myblog

0f6abeb9d85a7063298cd70082ac5e5a2f0d1624bae06619fd14dbaa0942b0e2

Once the containers are on private networks, restart them with the additional option -network myblog. This appears on the second to last line for both container commands.

$ docker run -d --name db --restart always \

    -v db_data:/var/lib/mysql -p 3306 -p 33060 \

    -e MYSQL_ROOT_PASSWORD=somewordpress \

    -e MYSQL_DATABASE=wordpress \

    -e MYSQL_USER=wordpress \

    -e MYSQL_PASSWORD=wordpress \

    --network myblog \

    mariadb:10

$ docker run -d --name wordpress --restart always \

    -v wp_data:/var/www/html -p 80:80 \

    -e WORDPRESS_DB_HOST=db \

    -e WORDPRESS_DB_USER=wordpress \

    -e WORDPRESS_DB_PASSWORD=wordpress \

    -e WORDPRESS_DB_NAME=wordpress \

    --network myblog \

    wordpress:latest

It’s good practice to verify that containers on other networks can’t access the private networks you created. To check this, start a new container and attempt to find the private containers you created.

$ docker run -it debian:latest 

root@7240f1e3ddab:/# ping db.myblog

ping: db.myblog: Name or service not known

Docker Compose
Docker Compose is an optional tool, provided by Docker, that makes using multiple containers easy. In most instances, Docker Compose is automatically installed during the installation process of Docker Desktop. If not, follow the instructions in 
Scenario two: Install the Compose plugin
 to install Docker Compose on your platform.

Docker Compose allows you to define a multiple-container setup in a single 
YAML
 format, called a Compose file. (YAML is a format for configuration files that’s designed to be both human- and computer-readable.) The Compose file communicates with Docker and identifies the containers you need and how you should configure them. The containers in a Compose file are called services. Let’s look at how you can use Compose to recreate the private networks from the  wordpress and db example. Run the following on your machine.

Create an empty folder and save the file below as docker-compose.yml.

version: '3.3'

services:

  db:

    image: mariadb:10

    volumes:

      - db_data:/var/lib/mysql

    restart: always

    environment:

      - MYSQL_ROOT_PASSWORD=somewordpress

      - MYSQL_DATABASE=wordpress

      - MYSQL_USER=wordpress

      - MYSQL_PASSWORD=wordpress

    networks:

      - myblog

    expose:

      - 3306

      - 33060

  wordpress:

    image: wordpress:latest

    volumes:

      - wp_data:/var/www/html

    ports:

      - 80:80

    networks:

      - myblog

    restart: always

    environment:

      - WORDPRESS_DB_HOST=db

      - WORDPRESS_DB_USER=wordpress

      - WORDPRESS_DB_PASSWORD=wordpress

      - WORDPRESS_DB_NAME=wordpress

volumes:

  db_data:

  wp_data:

networks:

  myblog:

Run the command docker compose up. This pulls up the images, creates two empty data volumes, and starts both services. The output from both services will intermingle on your screen.

The Compose file grants you control over how each service is configured, including:

Choosing the image

Setting environment variables

Mounting storage volumes

Exposing network ports

Pro tip: You can also express any option you pass to the docker run command as YAML in a Compose file.

A helpful third-party tool—that’s a fan favorite of programmers—that simplifies the process of converting existing Docker run commands into Docker Compose configurations is called Composerize. Refer to 
Composerize
 for additional information. You can test the above Docker command in the Composerize textbox. This command defines a db service similar to the one presented above. Remember, Composerize is just a tool, and unfortunately sometimes tools come and go. It’s best to understand and practice the process of converting an existing Docker run command into a Docker Compose configuration without the help of tools.

For additional information about the options you can put into a Compose file, view the 
Compose file overview documentation.

Additional Compose commands
Compose has additional commands when working with a single or multiple containers. Let’s look at some examples:

docker compose pull: This fetches the latest image for each service.

docker compose up: This creates the containers and starts the service.

docker compose down: This stops the service and deletes the container.

docker compose logs:  This displays the console logs from the container.

Key takeaways
Using multiple containers enables the adoption of a microservice architecture for your application. Separating a large application into smaller, independent parts allows for a more manageable approach to building, fixing, maintaining, and deploying each part of the application.

________________________

Docker and Google Cloud Platform (GCP) are two types of technologies that complement each other, allowing programmers to build, deploy, and manage containerized applications in the cloud.

In this reading, you will learn more about GCP, how to run Docker containers in GCP, and how to use Cloud Run.

Google Cloud Platform
GCP is a composition of all the cloud services provided by Google. These include:

Virtual machines

Containers

Computing

Hosting

Storage

Databases

Tools

Identity management

GCP is widely used by businesses, startup companies, developers, and organizations of all sizes across a variety of industries to help their users go digital.

How to run Docker containers in GCP
You can run containers two ways in the cloud using GCP. The first way is to start a virtual machine with Docker installed on it. Use the docker run command to create a container and start it. This is the same process for running Docker on any other host machine.

The second way is to use a service called Cloud Run. This serverless platform is managed by Google and allows you to launch containers without worrying about managing the underlying infrastructure. Cloud Run is simple and automated, and it’s designed to allow programmers to be more productive and move quickly.

An advantage of Cloud Run is that it allows you to deploy code written in any programming language if you can put the code into a container.

Use Cloud Run to deploy containers in GCP
Before you begin, sign into your Google account, or if you do not have one, create an account.

Open 
Cloud Run
.

Click Create service to display the form.

	In the form,

Select Deploy one revision from an existing container image.

Below the Container image URL text box, select Test with a sample container.

From the Region drop-down menu, select the region in which you want the service located.

Below Authentication, select Allow unauthenticated invocations.

Click Create to deploy the sample container image to Cloud Run and wait for the deployment to finish.

       3.  Select the displayed URL link to run the container.

Pro tip: Cloud Run helps keep costs down by only charging you for central processing unit (CPU) time while the container is running. It’s unlike running Docker on a virtual machine, for which you must keep the virtual machine on at all times—running up your bill.

Key takeaways
GCP supports Docker containers and provides services to support containerized applications. Integrating GCP and Docker allows developers and programmers to build, deploy, and run containers easily while being able to focus on the application logic.


__________________________

Arifacts and Testing

Build artifacts  
Build artifacts are items that you create during the build process. Your main artifact is your Docker container, if you’re working within a Dockerized application. All other items that you generate during the Docker image build process are also considered build artifacts. Some examples include:

Libraries

Documentation

Static files

Configuration files

Scripts


Build artifacts in Docker 
Build artifacts in Docker play a crucial role in the software development and deployment lifecycle. No matter what you create with code, you need to test it. You must test your code before deployment to ensure that you catch and correct all issues, defects, and errors. This is true whether your code is built as a Docker container or built the more “classic” way. The process to execute the testing varies based on the application and the programming language it’s written in.

Pro tip: It’s important to check that Docker built the container itself correctly if you are testing your code with a containerized application.

There are several types of software testing that you can execute with Docker containers:

Unit tests: These are small, granular tests written by the developer to test individual functions in the code. In Docker, unit tests are run directly on your codebase before the Docker image is built, ensuring the code is working as expected before being packaged.

Integration tests: These refer to testing an application or microservice in conjunction with the other services on which it relies. In a Dockerized environment, integration tests are run after the docker image is built and the container is running, testing how different components operate together inside the Docker container. 

End-to-end (E2E) tests: This type of testing simulates the behavior of a real user (e.g., by opening the browser and navigating through several pages). E2E tests are run against the fully deployed docker container, checking that the entire application stack with its various components and services functions correctly as a whole.

Performance tests: This type of testing identifies bottlenecks. Performance tests are run against the fully deployed Docker container and test various stresses and loads to ensure the application performs at expectations. 

Docker makes it easy to set up and tear down tests in a repeatable and predictable way. Testing Docker containers ensures the reliability, stability, and quality of the application running within them. By testing containers, you can discover bugs and compatibility and performance issues to ensure your application functions as intended.


How to test a Docker container 
Automated testing often requires supplying configuration files, data files, and test tools to the application you want to test, which unfortunately increases the size of your container. Instead, you can build a container just for testing, using your output artifact as a base image. Let’s take a look at an example: 

Let’s say a Python application uses pytest as a unit testing framework and Sphix to generate documentation. You can reuse your application container and build a new image that includes the tools on top.

FROM myapp:latest

RUN pip install pytest pydoc

WORKDIR /opt/myapp

CMD pytest .


This part of the code shows that you have a container that has both the application and the test framework in it. Now it’s time for you to run the test according to the framework you chose:

docker run -it myapp:test


You can mount data files for input or configuration as a volume when you create your test container:

docker run -it -v ./testdata:/data myapp:test


What should you do if your test fails? Hopefully it won’t, but if it does, don’t worry! You can troubleshoot it. First, open the shell inside the failed container and see if you can identify the problem. If the container is still running, use the docker exec command and the container ID:

docker exec -it c47da2b409a1 /bin/sh


If you get an error running the above command, that means the container exited. You can restart the container and then try again:

docker start c47da2b409a1

docker exec -it c47da2b409a1 /bin/sh


Sometimes the container is built with automatic health checks, and Docker might terminate the container before you can investigate the issue. If this happens, you can disable the health checks in your test container by adding the command HEALTHCHECK NONE to the Dockerfile.


If you run into this type of troubleshooting often, there are tools you can add to the Dockerfile so they’re always available. A couple of examples include:

jq: This is for examining JSON files.

curl, httpie, netcat: These are for testing network services.


If you want to add these tools to your test container, add it using the line below in bold:

FROM myapp:latest


RUN apt update && apt install -y jq curl netcat

RUN pip install pytest pydoc

WORKDIR /opt/myapp

CMD pytest .

Pro tip: You can never have too many automated tests. At a minimum, a good test suite will include both unit tests and integration tests.


Key takeaways
Running tests for your build artifacts and Docker containers helps ensure the reliability, stability, and quality of your work. You can never run too many tests. Tests are designed to catch bugs, identify compatibility issues, and find performance problems to assist in ensuring the application runs as intended.

